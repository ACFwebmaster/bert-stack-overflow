{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Training Tensorflow 2.0 Model on Azure Machine Learning Service\n",
    "\n",
    "## Overview of the part 1\n",
    "This notebook is Part 1 (Preparing Data and Model Training) of a four part workshop that demonstrates an end-to-end workflow using Tensorflow 2.0 on Azure Machine Learning service. The different components of the workshop are as follows:\n",
    "\n",
    "- Part 1: [Preparing Data and Model Training](https://github.com/microsoft/bert-stack-overflow/blob/master/1-Training/AzureServiceClassifier_Training.ipynb)\n",
    "- Part 2: [Inferencing and Deploying a Model](https://github.com/microsoft/bert-stack-overflow/blob/master/2-Inferencing/AzureServiceClassifier_Inferencing.ipynb)\n",
    "- Part 3: [Setting Up a Pipeline Using MLOps](https://github.com/microsoft/bert-stack-overflow/tree/master/3-ML-Ops)\n",
    "- Part 4: [Explaining Your Model Interpretability](https://github.com/microsoft/bert-stack-overflow/blob/master/4-Interpretibility/IBMEmployeeAttritionClassifier_Interpretability.ipynb)\n",
    "\n",
    "**This notebook will cover the following topics:**\n",
    "\n",
    "- Stackoverflow question tagging problem\n",
    "- Introduction to Transformer and BERT deep learning models\n",
    "- Introduction to Azure Machine Learning service\n",
    "- Preparing raw data for training using Apache Spark\n",
    "- Registering cleaned up training data as a Dataset\n",
    "- Debugging the model in Tensorflow 2.0 Eager Mode\n",
    "- Training the model on GPU cluster\n",
    "- Monitoring training progress with built-in Tensorboard dashboard \n",
    "- Automated search of best hyper-parameters of the model\n",
    "- Registering the trained model for future deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "This notebook is designed to be run in Azure ML Notebook VM. See [readme](https://github.com/microsoft/bert-stack-overflow/blob/master/README.md) file for instructions on how to create Notebook VM and open this notebook in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Azure Machine Learning Python SDK version\n",
    "\n",
    "This tutorial requires version 1.0.69 or higher. Let's check the version of the SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "\n",
    "print(\"Azure Machine Learning Python SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stackoverflow Question Tagging Problem \n",
    "In this workshop we will use powerful language understanding model to automatically route Stackoverflow questions to the appropriate support team on the example of Azure services.\n",
    "\n",
    "One of the key tasks to ensuring long term success of any Azure service is actively responding to related posts in online forums such as Stackoverflow. In order to keep track of these posts, Microsoft relies on the associated tags to direct questions to the appropriate support team. While Stackoverflow has different tags for each Azure service (azure-web-app-service, azure-virtual-machine-service, etc), people often use the generic **azure** tag. This makes it hard for specific teams to track down issues related to their product and as a result, many questions get left unanswered. \n",
    "\n",
    "**In order to solve this problem, we will build a model to classify posts on Stackoverflow with the appropriate Azure service tag.**\n",
    "\n",
    "We will be using a BERT (Bidirectional Encoder Representations from Transformers) model which was published by researchers at Google AI Reasearch. Unlike prior language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of natural language processing (NLP) tasks without substantial architecture modifications.\n",
    "\n",
    "## Why use BERT model?\n",
    "[Introduction of BERT model](https://arxiv.org/pdf/1810.04805.pdf) changed the world of NLP. Many NLP problems that before relied on specialized models to achive state of the art performance are now solved with BERT better and with more generic approach.\n",
    "\n",
    "If we look at the leaderboards on such popular NLP problems as GLUE and SQUAD, most of the top models are based on BERT:\n",
    "* [GLUE Benchmark Leaderboard](https://gluebenchmark.com/leaderboard/)\n",
    "* [SQuAD Benchmark Leaderboard](https://rajpurkar.github.io/SQuAD-explorer/)\n",
    "\n",
    "Recently, Allen Institue for AI announced new language understanding system called Aristo [https://allenai.org/aristo/](https://allenai.org/aristo/). The system has been developed for 20 years, but it's performance was stuck at 60% on 8th grade science test. The result jumped to 90% once researchers adopted BERT as core language understanding component. With BERT Aristo now solves the test with A grade.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Overview of How BERT model works\n",
    "\n",
    "The foundation of BERT model is Transformer model, which was introduced in [Attention Is All You Need paper](https://arxiv.org/abs/1706.03762). Before that event the dominant way of processing language was Recurrent Neural Networks (RNNs). Let's start our overview with RNNs.\n",
    "\n",
    "## RNNs\n",
    "\n",
    "RNNs were powerful way of processing language due to their ability to memorize its previous state and perform sophisticated inference based on that.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/400/1*L38xfe59H5tAgvuIjKoWPg.png\" alt=\"Drawing\" style=\"width: 100px;\"/>\n",
    "\n",
    "_Taken from [1](https://towardsdatascience.com/transformers-141e32e69591)_\n",
    "\n",
    "Applied to language translation task, the processing dynamics looked like this.\n",
    "\n",
    "![](https://miro.medium.com/max/1200/1*8GcdjBU5TAP36itWBcZ6iA.gif)\n",
    "_Taken from [2](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)_\n",
    "    \n",
    "But RNNs suffered from 2 disadvantes:\n",
    "1. Sequential computation put a limit on parallelization, which limited effectiveness of larger models.\n",
    "2. Long term relationships between words were harder to detect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "Transformers were designed to address these two limitations of RNNs.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2436/1*V2435M1u0tiSOz4nRBfl4g.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "_Taken from [3](http://jalammar.github.io/illustrated-transformer/)_\n",
    "\n",
    "In each Encoder layer Transformer performs Self-Attention operation which detects relationships between all word embeddings in one matrix multiplication operation. \n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2176/1*fL8arkEFVKA3_A7VBgapKA.gif\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "_Taken from [4](https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Model\n",
    "\n",
    "BERT is a very large network with multiple layers of Transformers (12 for BERT-base, and 24 for BERT-large). The model is first pre-trained on large corpus of text data (WikiPedia + books) using un-superwised training (predicting masked words in a sentence). During pre-training the model absorbs significant level of language understanding.\n",
    "\n",
    "<img src=\"http://jalammar.github.io/images/bert-output-vector.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "_Taken from [5](http://jalammar.github.io/illustrated-bert/)_\n",
    "\n",
    "Pre-trained network then can easily be fine-tuned to solve specific language task, like answering questions, or categorizing spam emails.\n",
    "\n",
    "<img src=\"http://jalammar.github.io/images/bert-classifier.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "_Taken from [5](http://jalammar.github.io/illustrated-bert/)_\n",
    "\n",
    "The end-to-end training process of the stackoverflow question tagging model looks like this:\n",
    "\n",
    "![](images/model-training-e2e.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Azure Machine Learning Service?\n",
    "Azure Machine Learning service is a cloud service that you can use to develop and deploy machine learning models. Using Azure Machine Learning service, you can track your models as you build, train, deploy, and manage them, all at the broad scale that the cloud provides.\n",
    "![](./images/aml-overview.png)\n",
    "\n",
    "\n",
    "#### How can we use it for training machine learning models?\n",
    "Training machine learning models, particularly deep neural networks, is often a time- and compute-intensive task. Once you've finished writing your training script and running on a small subset of data on your local machine, you will likely want to scale up your workload.\n",
    "\n",
    "To facilitate training, the Azure Machine Learning Python SDK provides a high-level abstraction, the estimator class, which allows users to easily train their models in the Azure ecosystem. You can create and use an Estimator object to submit any training code you want to run on remote compute, whether it's a single-node run or distributed training across a GPU cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect To Workspace\n",
    "\n",
    "The [workspace](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace(class)?view=azure-ml-py) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. The workspace holds all your experiments, compute targets, models, datastores, etc.\n",
    "\n",
    "You can [open ml.azure.com](https://ml.azure.com) to access your workspace resources through a graphical user interface of **Azure Machine Learning studio**.\n",
    "\n",
    "![](./images/aml-workspace.png)\n",
    "\n",
    "**You will be asked to login in the next step. Use your Microsoft AAD credentials.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Workspace name: BERT\nAzure region: australiaeast\nSubscription id: cdf3e529-94ee-4f54-a219-4720963fee3b\nResource group: DEV-CRM\n"
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "workspace = Workspace.get(\"BERT\", subscription_id=\"cdf3e529-94ee-4f54-a219-4720963fee3b\")\n",
    "#workspace = Workspace.from_config()\n",
    "print('Workspace name: ' + workspace.name, \n",
    "      'Azure region: ' + workspace.location, \n",
    "      'Subscription id: ' + workspace.subscription_id, \n",
    "      'Resource group: ' + workspace.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Compute Target\n",
    "\n",
    "A [compute target](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.computetarget?view=azure-ml-py) is a designated compute resource/environment where you run your training script or host your service deployment. This location may be your local machine or a cloud-based compute resource. Compute targets can be reused across the workspace for different runs and experiments. \n",
    "\n",
    "For this tutorial, we will create an auto-scaling [Azure Machine Learning Compute](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.compute.amlcompute?view=azure-ml-py) cluster, which is a managed-compute infrastructure that allows the user to easily create a single or multi-node compute. To create the cluster, we need to specify the following parameters:\n",
    "\n",
    "- `vm_size`: The is the type of GPUs that we want to use in our cluster. For this tutorial, we will use **Standard_NC12s_v3 (NVIDIA V100) GPU Machines** .\n",
    "- `idle_seconds_before_scaledown`: This is the number of seconds before a node will scale down in our auto-scaling cluster. We will set this to **6000** seconds. \n",
    "- `min_nodes`: This is the minimum numbers of nodes that the cluster will have. To avoid paying for compute while they are not being used, we will set this to **0** nodes.\n",
    "- `max_modes`: This is the maximum number of nodes that the cluster will scale up to. Will will set this to **2** nodes.\n",
    "\n",
    "**When jobs are submitted to the cluster it takes approximately 5 minutes to allocate new nodes** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Creating\nSucceeded\nAmlCompute wait for completion finished\nMinimum number of nodes requested have been provisioned\n"
    }
   ],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "\n",
    "cluster_name = 'v100cluster'\n",
    "compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2', \n",
    "                                                       idle_seconds_before_scaledown=6000,\n",
    "                                                       min_nodes=0, \n",
    "                                                       max_nodes=2)\n",
    "\n",
    "compute_target = ComputeTarget.create(workspace, cluster_name, compute_config)\n",
    "compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure our compute target was created successfully, we can check it's status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'currentNodeCount': 0,\n 'targetNodeCount': 0,\n 'nodeStateCounts': {'preparingNodeCount': 0,\n  'runningNodeCount': 0,\n  'idleNodeCount': 0,\n  'unusableNodeCount': 0,\n  'leavingNodeCount': 0,\n  'preemptedNodeCount': 0},\n 'allocationState': 'Steady',\n 'allocationStateTransitionTime': '2020-01-21T21:53:48.483000+00:00',\n 'errors': None,\n 'creationTime': '2020-01-21T21:53:45.443806+00:00',\n 'modifiedTime': '2020-01-21T21:54:02.971656+00:00',\n 'provisioningState': 'Succeeded',\n 'provisioningStateTransitionTime': None,\n 'scaleSettings': {'minNodeCount': 0,\n  'maxNodeCount': 2,\n  'nodeIdleTimeBeforeScaleDown': 'PT6000S'},\n 'vmPriority': 'Dedicated',\n 'vmSize': 'STANDARD_D2_V2'}"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_target.get_status().serialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the compute target has already been created, then you (and other users in your workspace) can directly run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_target = workspace.compute_targets['v100cluster']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data Using Apache Spark\n",
    "\n",
    "To train our model, we used the Stackoverflow data dump from [Stack exchange archive](https://archive.org/download/stackexchange). Since the Stackoverflow _posts_ dataset is 12GB, we prepared the data using [Apache Spark](https://spark.apache.org/) framework on a scalable Spark compute cluster in [Azure Databricks](https://azure.microsoft.com/en-us/services/databricks/). \n",
    "\n",
    "For the purpose of this tutorial, we have processed the data ahead of time and uploaded it to an [Azure Blob Storage](https://azure.microsoft.com/en-us/services/storage/blobs/) container. The full data processing notebook can be found in the _spark_ folder.\n",
    "\n",
    "* **ACTION**: Open and explore [data preparation notebook](spark/stackoverflow-data-prep.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Datastore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [Datastore](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.datastore.datastore?view=azure-ml-py) is used to store connection information to a central data storage. This allows you to access your storage without having to hard code this (potentially confidential) information into your scripts. \n",
    "\n",
    "In this tutorial, the data was been previously prepped and uploaded into a central [Blob Storage](https://azure.microsoft.com/en-us/services/storage/blobs/) container. We will register this container into our workspace as a datastore using a [shared access signature (SAS) token](https://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore, Dataset\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "# datastore_name = 'tfworld'\n",
    "# container_name = 'azureml-blobstore-7c6bdd88-21fa-453a-9c80-16998f02935f'\n",
    "# account_name = 'bert3308922571'\n",
    "# sas_token = '?sv=2019-02-02&ss=bfqt&srt=sco&sp=rl&se=2020-06-01T14:18:31Z&st=2019-11-05T07:18:31Z&spr=https&sig=Z4JmM0V%2FQzoFNlWS3a3vJxoGAx58iCz2HAWtmeLDbGE%3D'\n",
    "\n",
    "# datastore = Datastore.register_azure_blob_container(workspace=workspace, \n",
    "#                                                     datastore_name=datastore_name, \n",
    "#                                                     container_name=container_name,\n",
    "#                                                     account_name=account_name)\n",
    "\n",
    "blob_service_client = BlobServiceClient.from_connection_string(\"DefaultEndpointsProtocol=https;AccountName=bert3308922571;AccountKey=T2mfL1at6AMaL0Qstx/JjuWKrz8c/r8PayBdSRFPan3aIAeMJpuAx3biqojbKexx7aKBOAtSnpEtu8H+lj4FRw==;EndpointSuffix=core.windows.net\")\n",
    "\n",
    "with open(upload_file_path, \"rb\") as data:\n",
    "    blob_client.upload_blob(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the datastore has already been registered, then you (and other users in your workspace) can directly run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = workspace.datastores['tfworld']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if my data wasn't already hosted remotely?\n",
    "All workspaces also come with a blob container which is registered as a default datastore. This allows you to easily upload your own data to a remote storage location. You can access this datastore and upload files as follows:\n",
    "```\n",
    "datastore = workspace.get_default_datastore()\n",
    "ds.upload(src_dir='<LOCAL-PATH>', target_path='<REMOTE-PATH>')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Dataset\n",
    "\n",
    "Azure Machine Learning service supports first class notion of a Dataset. A [Dataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.dataset.dataset?view=azure-ml-py) is a resource for exploring, transforming and managing data in Azure Machine Learning. The following Dataset types are supported:\n",
    "\n",
    "* [TabularDataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.tabulardataset?view=azure-ml-py) represents data in a tabular format created by parsing the provided file or list of files.\n",
    "\n",
    "* [FileDataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.filedataset?view=azure-ml-py) references single or multiple files in datastores or from public URLs.\n",
    "\n",
    "First, we will use visual tools in Azure ML studio to register and explore our dataset as Tabular Dataset.\n",
    "\n",
    "* **ACTION**: Follow [create-dataset](images/create-dataset.ipynb) guide to create Tabular Dataset from our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use created dataset in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Cannot find dataset registered with name \"Stackoverflow dataset\" in the workspace.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-eaa42fc55d65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Get a dataset by name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtabular_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_by_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Stackoverflow dataset'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Load a TabularDataset into pandas DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\azureml\\data\\_loggerfactory.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_LoggerFactory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack_activity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivity_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_dimensions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mal\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'activity_info'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'error_code'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\azureml\\data\\abstract_dataset.py\u001b[0m in \u001b[0;36mget_by_name\u001b[1;34m(workspace, name, version)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mazureml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTabularDataset\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mazureml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFileDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \"\"\"\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAbstractDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_by_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[0mAbstractDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_track_lineage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\azureml\\data\\abstract_dataset.py\u001b[0m in \u001b[0;36m_get_by_name\u001b[1;34m(workspace, name, version)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msuccess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_dto_to_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Cannot find dataset registered with name \"Stackoverflow dataset\" in the workspace."
     ]
    }
   ],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "# Get a dataset by name\n",
    "tabular_ds = Dataset.get_by_name(workspace=workspace, name='Stackoverflow dataset')\n",
    "\n",
    "# Load a TabularDataset into pandas DataFrame\n",
    "df = tabular_ds.to_pandas_dataframe()\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Dataset using SDK\n",
    "\n",
    "In addition to UI we can register datasets using SDK. In this workshop we will register second type of Datasets using code - File Dataset. File Dataset allows specific folder in our datastore that contains our data files to be registered as a Dataset.\n",
    "\n",
    "There is a folder within our datastore called **azure-service-data** that contains all our training and testing data. We will register this as a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "DatasetValidationError",
     "evalue": "Cannot load any data from the specified path. Make sure the path is accessible and contains data.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mExecutionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\azureml\\data\\dataset_error_handling.py\u001b[0m in \u001b[0;36m_validate_has_data\u001b[1;34m(dataflow, error_message)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mdataflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverify_has_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     except (dataprep().api.dataflow.DataflowValidationError,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\azureml\\dataprep\\api\\_loggerfactory.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\azureml\\dataprep\\api\\dataflow.py\u001b[0m in \u001b[0;36mverify_has_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 845\u001b[1;33m         \u001b[0mprofile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_profile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    846\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mprofile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrow_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprofile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrow_count\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\azureml\\dataprep\\api\\dataflow.py\u001b[0m in \u001b[0;36m_get_profile\u001b[1;34m(self, include_stype_counts, number_of_histogram_bins, include_average_spaces_count, include_string_lengths)\u001b[0m\n\u001b[0;32m    547\u001b[0m                                            \u001b[0minclude_average_spaces_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 548\u001b[1;33m                                            include_string_lengths)\n\u001b[0m\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\azureml\\dataprep\\api\\dataprofile.py\u001b[0m in \u001b[0;36m_from_execution\u001b[1;34m(cls, engine_api, context, include_stype_counts, number_of_histogram_bins, include_average_spaces_count, include_string_lengths)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0minspector_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Inspector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_from_execution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengine_api\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable_inspector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\azureml\\dataprep\\api\\inspector.py\u001b[0m in \u001b[0;36m_from_execution\u001b[1;34m(cls, engine_api, context, inspector)\u001b[0m\n\u001b[0;32m    218\u001b[0m             \u001b[0moffset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m             row_count = _MAX_ROW_COUNT))\n\u001b[0m\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\azureml\\dataprep\\api\\_aml_helper.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(op_code, message, cancellation_token)\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[0mengine_api_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_environment_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchanged\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msend_message_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_code\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\azureml\\dataprep\\api\\engineapi\\api.py\u001b[0m in \u001b[0;36mexecute_inspector\u001b[1;34m(self, message_args, cancellation_token)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexecute_inspector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage_args\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtypedefinitions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExecuteInspectorCommonArguments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_token\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mCancellationToken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtypedefinitions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExecuteInspectorCommonResponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_message_channel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Engine.ExecuteInspector'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtypedefinitions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExecuteInspectorCommonResponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\azureml\\dataprep\\api\\engineapi\\engine.py\u001b[0m in \u001b[0;36msend_message\u001b[1;34m(self, op_code, message, cancellation_token)\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m'error'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m                     \u001b[0mraise_engine_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmessage_id\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\azureml\\dataprep\\api\\errorhandlers.py\u001b[0m in \u001b[0;36mraise_engine_error\u001b[1;34m(error_response)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'ActivityExecutionFailed'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0merror_code\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mExecutionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_response\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;34m'UnableToPreviewDataSource'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0merror_code\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mExecutionError\u001b[0m: The provided path is not valid or the files could not be accessed.(No files were found using path provided. Please make sure the path you've specified is correct, files exist and can be accessed.)|session_id=9845fb86-0aa6-4d8b-96b9-700d5433f880",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mDatasetValidationError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-a26f92ca05c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mazure_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatastore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'C:/Users/phill/azure-ml/azure-service-classifier/data-shared-inbox'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m azure_dataset = azure_dataset.register(workspace=workspace,\n\u001b[0;32m      4\u001b[0m                                        \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Azure Services Dataset'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                        description='Dataset containing azure related posts on Stackoverflow')\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\azureml\\data\\_loggerfactory.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_LoggerFactory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack_activity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivity_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_dimensions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mal\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'activity_info'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'error_code'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\azureml\\data\\dataset_factory.py\u001b[0m in \u001b[0;36mfrom_files\u001b[1;34m(path, validate)\u001b[0m\n\u001b[0;32m    444\u001b[0m         \u001b[0mdataflow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataprep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path_to_get_files_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_normalize_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m             _validate_has_data(dataflow, 'Cannot load any data from the specified path. ' +\n\u001b[0m\u001b[0;32m    447\u001b[0m                                          'Make sure the path is accessible and contains data.')\n\u001b[0;32m    448\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mFileDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataflow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\azureml\\data\\dataset_error_handling.py\u001b[0m in \u001b[0;36m_validate_has_data\u001b[1;34m(dataflow, error_message)\u001b[0m\n\u001b[0;32m     46\u001b[0m     except (dataprep().api.dataflow.DataflowValidationError,\n\u001b[0;32m     47\u001b[0m             dataprep().api.errorhandlers.ExecutionError):\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mDatasetValidationError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDatasetValidationError\u001b[0m: Cannot load any data from the specified path. Make sure the path is accessible and contains data."
     ]
    }
   ],
   "source": [
    "azure_dataset = Dataset.File.from_files(path=(datastore, 'C:/Users/phill/azure-ml/azure-service-classifier/data-shared-inbox'))\n",
    "\n",
    "azure_dataset = azure_dataset.register(workspace=workspace,\n",
    "                                       name='Azure Services Dataset',\n",
    "                                       description='Dataset containing azure related posts on Stackoverflow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the dataset has already been registered, then you (and other users in your workspace) can directly run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_dataset = workspace.datasets['Azure Services Dataset']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Training Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workshop the training code is provided in [train.py](./train.py) and [model.py](./model.py) files. The model is based on popular [huggingface/transformers](https://github.com/huggingface/transformers) libary. Transformers library provides performant implementation of BERT model with high level and easy to use APIs based on Tensorflow 2.0.\n",
    "\n",
    "![](https://raw.githubusercontent.com/huggingface/transformers/master/docs/source/imgs/transformers_logo_name.png)\n",
    "\n",
    "*  **ACTION**: Explore _train.py_ and _model.py_ using [Azure ML studio > Notebooks tab](images/azuremlstudio-notebooks-explore.png)\n",
    "* NOTE: You can also explore the files using Jupyter or Jupyter Lab UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Locally\n",
    "\n",
    "Let's try running the script locally to make sure it works before scaling up to use our compute cluster. To do so, you will need to install the transformers libary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Collecting transformers==2.0.0\n  Downloading https://files.pythonhosted.org/packages/66/99/ca0e4c35ccde7d290de3c9c236d5629d1879b04927e5ace9bd6d9183e236/transformers-2.0.0-py3-none-any.whl (290kB)\nRequirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==2.0.0) (1.16.2)\nRequirement already satisfied: boto3 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==2.0.0) (1.11.6)\nRequirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==2.0.0) (2.22.0)\nCollecting sacremoses (from transformers==2.0.0)\n  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\nCollecting regex (from transformers==2.0.0)\n  Downloading https://files.pythonhosted.org/packages/87/61/a3d8311dccec246605983a39b074eb175338f21cba774db0163e5ad0a139/regex-2020.1.8-cp37-cp37m-win_amd64.whl (271kB)\nRequirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==2.0.0) (4.36.1)\nCollecting sentencepiece (from transformers==2.0.0)\n  Downloading https://files.pythonhosted.org/packages/61/c5/e7e2f45c076097ac1a58b21288be25ae4eb4044be899e6c04cd897a00f15/sentencepiece-0.1.85-cp37-cp37m-win_amd64.whl (1.2MB)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->transformers==2.0.0) (0.9.4)\nRequirement already satisfied: botocore<1.15.0,>=1.14.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->transformers==2.0.0) (1.14.6)\nRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->transformers==2.0.0) (0.3.1)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==2.0.0) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==2.0.0) (1.24.2)\nRequirement already satisfied: idna<2.9,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==2.0.0) (2.8)\nRequirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==2.0.0) (2019.9.11)\nRequirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from sacremoses->transformers==2.0.0) (1.12.0)\nRequirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from sacremoses->transformers==2.0.0) (7.0)\nRequirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from sacremoses->transformers==2.0.0) (0.13.2)\nRequirement already satisfied: docutils<0.16,>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.15.0,>=1.14.6->boto3->transformers==2.0.0) (0.15.2)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.15.0,>=1.14.6->boto3->transformers==2.0.0) (2.8.0)\nBuilding wheels for collected packages: sacremoses\n  Building wheel for sacremoses (setup.py): started\n  Building wheel for sacremoses (setup.py): finished with status 'done'\n  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp37-none-any.whl size=885394 sha256=f095f3482f6c757823ceb6ea26e2fa6b5b0ddabbb824d629e99fceefa2bc8119\n  Stored in directory: C:\\Users\\phill\\AppData\\Local\\pip\\Cache\\wheels\\6d\\ec\\1a\\21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\nSuccessfully built sacremoses\nInstalling collected packages: regex, sacremoses, sentencepiece, transformers\nSuccessfully installed regex-2020.1.8 sacremoses-0.0.38 sentencepiece-0.1.85 transformers-2.0.0\nNote: you may need to restart the kernel to use updated packages.\n"
    }
   ],
   "source": [
    "%pip install transformers==2.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have taken a small partition of the dataset and included it in this repository. Let's take a quick look at the format of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'C:/Users/phill/azure-ml/bert-stack-overflow/1-Training/data-shared-inbox/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>533</td>\n      <td>I dont agree with you</td>\n      <td>no reply required</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>512</td>\n      <td>Wow what an amazing opportunity Beth. I would ...</td>\n      <td>no reply required</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>623</td>\n      <td>Dear Kelly.\\n\\nThank you for your email.\\n\\nIt...</td>\n      <td>no reply required</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>598</td>\n      <td>This is an automatic response to thank you for...</td>\n      <td>no reply required</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>632</td>\n      <td>Hello Christian\\n\\nYou need to think wider tha...</td>\n      <td>no reply required</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "     0                                                  1                  2\n0  533                              I dont agree with you  no reply required\n1  512  Wow what an amazing opportunity Beth. I would ...  no reply required\n2  623  Dear Kelly.\\n\\nThank you for your email.\\n\\nIt...  no reply required\n3  598  This is an automatic response to thank you for...  no reply required\n4  632  Hello Christian\\n\\nYou need to think wider tha...  no reply required"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "data = pd.read_csv(os.path.join(data_dir, 'train.csv'), header=None)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know what the data looks like, let's test out our script!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} train.py --data_dir {data_dir} --max_seq_length 128 --batch_size 16 --learning_rate 3e-5 --steps_per_epoch 5 --num_epochs 1 --export_dir ../outputs/model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging in TensorFlow 2.0 Eager Mode\n",
    "\n",
    "Eager mode is new feature in TensorFlow 2.0 which makes understanding and debugging models easy. Let's start by configuring our remote debugging environment.\n",
    "\n",
    "#### Configure VS Code Remote connection to Notebook VM\n",
    "\n",
    "* **ACTION**: Install [Microsoft VS Code](https://code.visualstudio.com/) on your local machine.\n",
    "\n",
    "* **ACTION**: Follow this [configuration guide](https://github.com/danielsc/azureml-debug-training/blob/master/Setting%20up%20VSCode%20Remote%20on%20an%20AzureML%20Notebook%20VM.md) to setup VS Code Remote connection to Notebook VM.\n",
    "\n",
    "#### Debug training code using step-by-step debugger\n",
    "\n",
    "* **ACTION**: Open Remote VS Code session to your Notebook VM.\n",
    "* **ACTION**: Open file `/home/azureuser/cloudfiles/code/<username>/bert-stack-overflow/1-Training/train_eager.py`.\n",
    "* **ACTION**: Set break point in the file and start Python debugging session. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a CPU machine training on a full dataset will take approximatly 1.5 hours. Although it's a small dataset, it still takes a long time. Let's see how we can speed up the training by using latest NVidia V100 GPUs in the Azure cloud. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Experiment\n",
    "\n",
    "Now that we have our compute target, dataset, and training script working locally, it is time to scale up so that the script can run faster. We will start by creating an [experiment](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment.experiment?view=azure-ml-py). An experiment is a grouping of many runs from a specified script. All runs in this tutorial will be performed under the same experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Workspace name: BERT\nAzure region: australiaeast\nSubscription id: cdf3e529-94ee-4f54-a219-4720963fee3b\nResource group: DEV-CRM\n"
    }
   ],
   "source": [
    "\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.get(\"BERT\", subscription_id=\"cdf3e529-94ee-4f54-a219-4720963fee3b\")\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<table style=\"width:100%\"><tr><th>Name</th><th>Workspace</th><th>Report Page</th><th>Docs Page</th></tr><tr><td>azure-service-classifier</td><td>BERT</td><td><a href=\"https://ml.azure.com/experiments/azure-service-classifier?wsid=/subscriptions/cdf3e529-94ee-4f54-a219-4720963fee3b/resourcegroups/DEV-CRM/workspaces/BERT\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment.Experiment?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>",
      "text/plain": "Experiment(Name: azure-service-classifier,\nWorkspace: BERT)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Experiment\n",
    "workspace=Workspace.get(\"BERT\", subscription_id=\"cdf3e529-94ee-4f54-a219-4720963fee3b\")\n",
    "experiment_name = 'azure-service-classifier' \n",
    "experiment = Experiment(workspace, name=experiment_name)\n",
    "experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create TensorFlow Estimator\n",
    "\n",
    "The Azure Machine Learning Python SDK Estimator classes allow you to easily construct run configurations for your experiments. They allow you too define parameters such as the training script to run, the compute target to run it on, framework versions, additional package requirements, etc. \n",
    "\n",
    "You can also use a generic [Estimator](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.estimator.estimator?view=azure-ml-py) to submit training scripts that use any learning framework you choose.\n",
    "\n",
    "For popular libaries like PyTorch and Tensorflow you can use their framework specific estimators. We will use the [TensorFlow Estimator](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.dnn.tensorflow?view=azure-ml-py) for our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.dnn import TensorFlow\n",
    "\n",
    "script_params = {\n",
    "    # to mount files referenced by mnist dataset\n",
    "    '--data_dir': './1-Training/data-shared-inbox',\n",
    "    '--max_seq_length': 128,\n",
    "    '--batch_size': 32,\n",
    "    '--learning_rate': 3e-5,\n",
    "    '--steps_per_epoch': 150,\n",
    "    '--num_epochs': 3,\n",
    "    '--export_dir':'./outputs'\n",
    "}\n",
    "\n",
    "estimator1 = TensorFlow(source_directory='C:/Users/phill/azure-ml/bert-stack-overflow',\n",
    "                        entry_script='./1-Training/train.py',\n",
    "                        compute_target=\"local\",\n",
    "                        script_params = script_params,\n",
    "                        framework_version='2.0',\n",
    "                        pip_packages=['transformers==2.0.0', 'azureml-dataprep[fuse,pandas]==1.1.29'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick description for each of the parameters we have just defined:\n",
    "\n",
    "- `source_directory`: This specifies the root directory of our source code. \n",
    "- `entry_script`: This specifies the training script to run. It should be relative to the source_directory.\n",
    "- `compute_target`: This specifies to compute target to run the job on. We will use the one created earlier.\n",
    "- `script_params`: This specifies the input parameters to the training script. Please note:\n",
    "\n",
    "    1) *azure_dataset.as_named_input('azureservicedata').as_mount()* mounts the dataset to the remote compute and provides the path to the dataset on our datastore. \n",
    "    \n",
    "    2) All outputs from the training script must be outputted to an './outputs' directory as this is the only directory that will be saved to the run. \n",
    "    \n",
    "    \n",
    "- `framework_version`: This specifies the version of TensorFlow to use. Use Tensorflow.get_supported_verions() to see all supported versions.\n",
    "- `use_gpu`: This will use the GPU on the compute target for training if set to True.\n",
    "- `pip_packages`: This allows you to define any additional libraries to install before training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Submit First Run \n",
    "\n",
    "We can now train our model by submitting the estimator object as a [run](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.run.run?view=azure-ml-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "run1 = experiment.submit(estimator1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the current status of the run and stream the logs from within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee8e9f84f30447e9b4e7b3331fb28e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"loading\": true}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You cancel a run at anytime which will stop the run and scale down the nodes in the compute target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run1.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we wait for the run to complete, let's go over how a Run is executed in Azure Machine Learning.\n",
    "\n",
    "![](./images/aml-run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Add Metrics Logging\n",
    "\n",
    "So we were able to clone a Tensorflow 2.0 project and run it without any changes. However, with larger scale projects we would want to log some metrics in order to make it easier to monitor the performance of our model. \n",
    "\n",
    "We can do this by adding a few lines of code into our training script:\n",
    "\n",
    "```python\n",
    "# 1) Import SDK Run object\n",
    "from azureml.core.run import Run\n",
    "\n",
    "# 2) Get current service context\n",
    "run = Run.get_context()\n",
    "\n",
    "# 3) Log the metrics that we want\n",
    "run.log('val_accuracy', float(logs.get('val_accuracy')))\n",
    "run.log('accuracy', float(logs.get('accuracy')))\n",
    "```\n",
    "We've created a *train_logging.py* script that includes logging metrics as shown above. \n",
    "\n",
    "*  **ACTION**: Explore _train_logging.py_ using [Azure ML studio > Notebooks tab](images/azuremlstudio-notebooks-explore.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can submit this run in the same way that we did before. \n",
    "\n",
    "*Since our cluster can scale automatically to two nodes, we can run this job simultaneously with the previous one.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_params = {\n",
    "    # to mount files referenced by mnist dataset\n",
    "    '--data_dir': './1-Training/data-shared-inbox',\n",
    "    '--max_seq_length': 128,\n",
    "    '--batch_size': 32,\n",
    "    '--learning_rate': 3e-5,\n",
    "    '--steps_per_epoch': 150,\n",
    "    '--num_epochs': 3,\n",
    "    '--export_dir':'./1-Training/outputs/model'\n",
    "}\n",
    "\n",
    "estimator2 = TensorFlow(source_directory='C:/Users/phill/azure-ml/bert-stack-overflow',\n",
    "                        entry_script='./1-Training/train_logging.py',\n",
    "                        compute_target=\"local\",\n",
    "                        script_params = script_params,\n",
    "                        framework_version='2.0',\n",
    "                        pip_packages=['transformers==2.0.0', 'azureml-dataprep[fuse,pandas]==1.1.29'])\n",
    "\n",
    "run2 = experiment.submit(estimator2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we view the current details of the run, you will notice that the metrics will be logged into graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0dec68111d48bfbe1825f67aafda7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"loading\": true}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Monitoring metrics with Tensorboard\n",
    "\n",
    "Tensorboard is a popular Deep Learning Training visualization tool and it's built-in into TensorFlow framework. We can easily add tracking of the metrics in Tensorboard format by adding Tensorboard callback to the **fit** function call.\n",
    "```python\n",
    "    # Add callback to record Tensorboard events\n",
    "    model.fit(train_dataset, epochs=FLAGS.num_epochs, \n",
    "              steps_per_epoch=FLAGS.steps_per_epoch, validation_data=valid_dataset, \n",
    "              callbacks=[\n",
    "                  AmlLogger(),\n",
    "                  tf.keras.callbacks.TensorBoard(update_freq='batch')]\n",
    "             )\n",
    "```\n",
    "\n",
    "#### Launch Tensorboard\n",
    "Azure ML service provides built-in integration with Tensorboard through **tensorboard** package.\n",
    "\n",
    "While the run is in progress (or after it has completed), we can start Tensorboard with the run as its target, and it will begin streaming logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "http://localhost:6006/\n"
    },
    {
     "data": {
      "text/plain": "'http://localhost:6006/'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.tensorboard import Tensorboard\n",
    "\n",
    "# The Tensorboard constructor takes an array of runs, so be sure and pass it in as a single-element array here\n",
    "tb = Tensorboard([run2])\n",
    "\n",
    "# If successful, start() returns a string with the URI of the instance.\n",
    "tb.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Tensorboard\n",
    "When you're done, make sure to call the stop() method of the Tensorboard object, or it will stay running even after your job completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the model performance\n",
    "\n",
    "Last training run produced model of decent accuracy. Let's test it out and see what it does. First, let's check what files our latest training run produced and download the model files.\n",
    "\n",
    "#### Download model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run2.get_file_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run2.download_files(prefix='outputs/model')\n",
    "\n",
    "# If you haven't finished training the model then just download pre-made model from datastore\n",
    "datastore.download('./',prefix=\"azure-service-classifier/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate the model\n",
    "\n",
    "Next step is to import our model class and instantiate fine-tuned model from the model file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import TFBertForMultiClassification\n",
    "from transformers import BertTokenizer\n",
    "import tensorflow as tf\n",
    "def encode_example(text, max_seq_length):\n",
    "    # Encode inputs using tokenizer\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_seq_length\n",
    "        )\n",
    "    input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding_length = max_seq_length - len(input_ids)\n",
    "    input_ids = input_ids + ([0] * padding_length)\n",
    "    attention_mask = attention_mask + ([0] * padding_length)\n",
    "    token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "    \n",
    "    return input_ids, attention_mask, token_type_ids\n",
    "    \n",
    "labels = ['azure-web-app-service', 'azure-storage', 'azure-devops', 'azure-virtual-machine', 'azure-functions']\n",
    "# Load model and tokenizer\n",
    "loaded_model = TFBertForMultiClassification.from_pretrained('azure-service-classifier/model', num_labels=len(labels))\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "print(\"Model loaded from disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define prediction function\n",
    "\n",
    "Using the model object we can interpret new questions and predict what Azure service they talk about. To do that conveniently we'll define **predict** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def predict(question):\n",
    "    input_ids, attention_mask, token_type_ids = encode_example(question, 128)\n",
    "    predictions = loaded_model.predict({\n",
    "        'input_ids': tf.convert_to_tensor([input_ids], dtype=tf.int32),\n",
    "        'attention_mask': tf.convert_to_tensor([attention_mask], dtype=tf.int32),\n",
    "        'token_type_ids': tf.convert_to_tensor([token_type_ids], dtype=tf.int32)\n",
    "    })\n",
    "    prediction = labels[predictions[0].argmax().item()]\n",
    "    probability = predictions[0].max()\n",
    "    result = {\n",
    "        'prediction': str(labels[predictions[0].argmax().item()]),\n",
    "        'probability': str(predictions[0].max())\n",
    "    }\n",
    "    print('Prediction: {}'.format(prediction))\n",
    "    print('Probability: {}'.format(probability))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiement with our new model\n",
    "\n",
    "Now we can easily test responses of the model to new inputs. \n",
    "*  **ACTION**: Invent yout own input for one of the 5 services our model understands: 'azure-web-app-service', 'azure-storage', 'azure-devops', 'azure-virtual-machine', 'azure-functions'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Route question\n",
    "predict(\"How can I specify Service Principal in devops pipeline when deploying virtual machine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now more tricky cae - the opposite\n",
    "predict(\"How can virtual machine trigger devops pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Training Across Multiple GPUs\n",
    "\n",
    "Distributed training allows us to train across multiple nodes if your cluster allows it. Azure Machine Learning service helps manage the infrastructure for training distributed jobs. All we have to do is add the following parameters to our estimator object in order to enable this:\n",
    "\n",
    "- `node_count`: The number of nodes to run this job across. Our cluster has a maximum node limit of 2, so we can set this number up to 2.\n",
    "- `process_count_per_node`: The number of processes to enable per node. The nodes in our cluster have 2 GPUs each. We will set this value to 2 which will allow us to distribute the load on both GPUs. Using multi-GPUs nodes is benefitial as communication channel bandwidth on local machine is higher.\n",
    "- `distributed_training`: The backend to use for our distributed job. We will be using an MPI (Message Passing Interface) backend which is used by Horovod framework.\n",
    "\n",
    "We use [Horovod](https://github.com/horovod/horovod), which is a framework that allows us to easily modifying our existing training script to be run across multiple nodes/GPUs. The distributed training script is saved as *train_horovod.py*.\n",
    "\n",
    "*  **ACTION**: Explore _train_horovod.py_ using [Azure ML studio > Notebooks tab](images/azuremlstudio-notebooks-explore.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can submit this run in the same way that we did with the others, but with the additional parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.train.dnn import Mpi\n",
    "\n",
    "estimator3 = TensorFlow(source_directory='./',\n",
    "                        entry_script='train_horovod.py',compute_target=compute_target,\n",
    "                        script_params = {\n",
    "                              '--data_dir': azure_dataset.as_named_input('azureservicedata').as_mount(),\n",
    "                              '--max_seq_length': 128,\n",
    "                              '--batch_size': 32,\n",
    "                              '--learning_rate': 3e-5,\n",
    "                              '--steps_per_epoch': 150,\n",
    "                              '--num_epochs': 3,\n",
    "                              '--export_dir':'./outputs/model'\n",
    "                        },\n",
    "                        framework_version='2.0',\n",
    "                        node_count=1,\n",
    "                        distributed_training=Mpi(process_count_per_node=2),\n",
    "                        use_gpu=True,\n",
    "                        pip_packages=['transformers==2.0.0', 'azureml-dataprep[fuse,pandas]==1.1.29'])\n",
    "\n",
    "run3 = experiment.submit(estimator3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we can view the current details of the run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the run completes note the time it took. It should be around 5 minutes. As you can see, by moving to the cloud GPUs and using distibuted training we managed to reduce training time of our model from more than an hour to 5 minutes. This greatly improves speed of experimentation and innovation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Hyperparameters Using Hyperdrive\n",
    "\n",
    "So far we have been putting in default hyperparameter values, but in practice we would need tune these values to optimize the performance. Azure Machine Learning service provides many methods for tuning hyperparameters using different strategies.\n",
    "\n",
    "The first step is to choose the parameter space that we want to search. We have a few choices to make here :\n",
    "\n",
    "- **Parameter Sampling Method**: This is how we select the combinations of parameters to sample. Azure Machine Learning service offers [RandomParameterSampling](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.randomparametersampling?view=azure-ml-py), [GridParameterSampling](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.gridparametersampling?view=azure-ml-py), and [BayesianParameterSampling](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.bayesianparametersampling?view=azure-ml-py). We will use the `GridParameterSampling` method.\n",
    "- **Parameters To Search**: We will be searching for optimal combinations of `learning_rate` and `num_epochs`.\n",
    "- **Parameter Expressions**: This defines the [functions that can be used to describe a hyperparameter search space](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.parameter_expressions?view=azure-ml-py), which can be discrete or continuous. We will be using a `discrete set of choices`.\n",
    "\n",
    "The following code allows us to define these options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import GridParameterSampling\n",
    "from azureml.train.hyperdrive.parameter_expressions import choice\n",
    "\n",
    "\n",
    "param_sampling = GridParameterSampling( {\n",
    "        '--learning_rate': choice(3e-5, 3e-4),\n",
    "        '--num_epochs': choice(3, 4)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to a define how we want to measure our performance. We do so by specifying two classes:\n",
    "\n",
    "- **[PrimaryMetricGoal](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.primarymetricgoal?view=azure-ml-py)**: We want to `MAXIMIZE` the `val_accuracy` that is logged in our training script.\n",
    "- **[BanditPolicy](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.banditpolicy?view=azure-ml-py)**: A policy for early termination so that jobs which don't show promising results will stop automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import BanditPolicy\n",
    "from azureml.train.hyperdrive import PrimaryMetricGoal\n",
    "\n",
    "primary_metric_name='val_accuracy'\n",
    "primary_metric_goal=PrimaryMetricGoal.MAXIMIZE\n",
    "\n",
    "early_termination_policy = BanditPolicy(slack_factor = 0.1, evaluation_interval=1, delay_evaluation=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an estimator as usual, but this time without the script parameters that we are planning to search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator4 = TensorFlow(source_directory='./',\n",
    "                        entry_script='train_logging.py',\n",
    "                        compute_target=compute_target,\n",
    "                        script_params = {\n",
    "                              '--data_dir': azure_dataset.as_named_input('azureservicedata').as_mount(),\n",
    "                              '--max_seq_length': 128,\n",
    "                              '--batch_size': 32,\n",
    "                              '--steps_per_epoch': 150,\n",
    "                              '--export_dir':'./outputs/model',\n",
    "                        },\n",
    "                        framework_version='2.0',\n",
    "                        use_gpu=True,\n",
    "                        pip_packages=['transformers==2.0.0', 'azureml-dataprep[fuse,pandas]==1.1.29'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we add all our parameters in a [HyperDriveConfig](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py) class and submit it as a run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import HyperDriveConfig\n",
    "\n",
    "hyperdrive_run_config = HyperDriveConfig(estimator=estimator4,\n",
    "                                         hyperparameter_sampling=param_sampling, \n",
    "                                         policy=early_termination_policy,\n",
    "                                         primary_metric_name=primary_metric_name, \n",
    "                                         primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                                         max_total_runs=10,\n",
    "                                         max_concurrent_runs=2)\n",
    "\n",
    "run4 = experiment.submit(hyperdrive_run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we view the details of our run this time, we will see information and metrics for every run in our hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run4).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the best run based on our defined metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_run = run4.get_best_run_by_primary_metric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Model\n",
    "\n",
    "A registered [model](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model(class)?view=azure-ml-py) is a reference to the directory or file that make up your model. After registering a model, you and other people in your workspace can easily gain access to and deploy your model without having to run the training script again. \n",
    "\n",
    "We need to define the following parameters to register a model:\n",
    "\n",
    "- `model_name`: The name for your model. If the model name already exists in the workspace, it will create a new version for the model.\n",
    "- `model_path`: The path to where the model is stored. In our case, this was the *export_dir* defined in our estimators.\n",
    "- `description`: A description for the model.\n",
    "\n",
    "Let's register the best run from our hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = best_run.register_model(model_name='azure-service-classifier', \n",
    "                                model_path='./outputs/model',\n",
    "                                datasets=[('train, test, validation data', azure_dataset)],\n",
    "                                description='BERT model for classifying azure services on stackoverflow posts.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have registered the model with Dataset reference. \n",
    "* **ACTION**: Check dataset to model link in **Azure ML studio > Datasets tab > Azure Service Dataset**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [next tutorial](), we will perform inferencing on this model and deploy it to a web service."
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}